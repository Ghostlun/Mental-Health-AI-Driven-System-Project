{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mental Health Counsel Chatbot\n",
        "\n",
        "Kaggle Notebook: Mental Health Counsel Chatbot\n",
        "Description: Provides mental health counseling data, which we used to supplement information from the primary dataset and align topics for consistent categorization.\n",
        "Mental Health Synthetic Dataset\n",
        "\n",
        "Kaggle Dataset: Mental Health Synthetic Dataset\n",
        "Description: This primary dataset contains synthetic data on mental health symptoms, demographics, and treatment, forming the basis for model training and recommendation generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_BfAu_XPWub",
        "outputId": "693f18da-464b-47dc-8e40-b70491dae910"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['User ID', 'Age', 'Gender', 'Symptoms', 'Duration (weeks)',\n",
              "       'Previous Diagnosis', 'Therapy History', 'Medication',\n",
              "       'Diagnosis / Condition', 'Suggested Therapy', 'Self-care Advice',\n",
              "       'Urgency Level', 'Mood', 'Stress Level'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "mental_df = pd.read_csv(\"mental_health.csv\")\n",
        "mental_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['User ID', 'Age', 'Gender', 'Symptoms', 'Duration', 'Prev_Diagnosis',\n",
              "       'Therapy_History', 'Medication', 'Diagnosis', 'Suggested_Therapy',\n",
              "       'Self_Care_Advice', 'Urgency_Level', 'Mood', 'Stress_Level'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Renmae columns: easy to follow up.\n",
        "mental_df = mental_df.rename(columns= {\n",
        "    'Diagnosis / Condition' : 'Diagnosis',\n",
        "    'Self-care Advice': 'Self_Care_Advice',\n",
        "    'Therapy History' : 'Therapy_History',\n",
        "    'Stress Level' : 'Stress_Level',\n",
        "    'Urgency Level' : 'Urgency_Level',\n",
        "    'Suggested Therapy': 'Suggested_Therapy',\n",
        "    'Duration (weeks)': 'Duration',\n",
        "    'Previous Diagnosis': 'Prev_Diagnosis'\n",
        "})\n",
        "\n",
        "# Check renamed columns\n",
        "mental_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1rl9DKaPRJ4-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Symtoms unique values \n",
            " ['feeling anxious' 'excessive worry' 'trouble sleeping'\n",
            " 'loss of interest in activities' 'panic attacks' 'lack of concentration'\n",
            " 'feeling irritable' 'feeling sad' 'feeling overwhelmed']\n",
            "therapy_unique_values \n",
            " ['Support Groups' 'Cognitive Behavioral Therapy' 'Psychotherapy'\n",
            " 'Mindfulness-Based Therapy' 'No Therapy Needed']\n",
            "diagnosis_unique_values \n",
            " ['Panic Disorder' 'Depression' 'Anxiety' 'Burnout' 'Stress']\n"
          ]
        }
      ],
      "source": [
        "symptoms_unique_values = mental_df['Symptoms'].unique()\n",
        "therapy_unique_values = mental_df['Suggested_Therapy'].unique()\n",
        "diagnosis_unique_values = mental_df['Diagnosis'].unique()\n",
        "\n",
        "print(\"Symtoms unique values \\n\", symptoms_unique_values)\n",
        "print(\"therapy_unique_values \\n\", therapy_unique_values)\n",
        "print(\"diagnosis_unique_values \\n\", diagnosis_unique_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A label encoder is a technique that converts non-numerical data into numerical values, \n",
        "which is useful for machine learning and data analysis. <br>\n",
        "It's often used when working with categorical data, such as ordinal data, \n",
        "where there's a hierarchy among the values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "# Target supposed to be Target Supposed.\n",
        "#  th\n",
        "def build_self_test_self_care_advice():\n",
        "    le_diagnosis = LabelEncoder()\n",
        "    le_symtoms = LabelEncoder()\n",
        "    le_self_care = LabelEncoder()\n",
        "    le_therapy = LabelEncoder()\n",
        "    # mental_df['Duration'] is already Int style, we don't have to encode it\n",
        "    \n",
        "    mental_df['Diagnosis_encoded'] = le_diagnosis.fit_transform(mental_df['Diagnosis'])\n",
        "    mental_df['Symptoms_encoded'] = le_symtoms.fit_transform(mental_df['Symptoms'])\n",
        "    mental_df['Self_Care_Advice_encoded'] = le_self_care.fit_transform(mental_df['Self_Care_Advice'])\n",
        "    mental_df['Suggested_Therapy_encoded'] = le_therapy.fit_transform(mental_df['Suggested_Therapy'])\n",
        "\n",
        "    # Training Data\n",
        "    X = mental_df[['Diagnosis_encoded', 'Symptoms_encoded']]\n",
        "    y_self_care = mental_df['Self_Care_Advice_encoded']\n",
        "    y_therapy = mental_df['Suggested_Therapy_encoded']\n",
        "\n",
        "    X_train, X_test, y_self_care_train, y_self_care_test, y_therapy_train, y_therapy_test = train_test_split(X, y_self_care, y_therapy, test_size=0.2, random_state=42)\n",
        "    # Train models for Self Care Advice and Suggested Therapy\n",
        "    model_self_care = RandomForestClassifier()\n",
        "    model_therapy = RandomForestClassifier()\n",
        "\n",
        "    model_self_care.fit(X_train, y_self_care_train)\n",
        "    model_therapy.fit(X_train, y_therapy_train)\n",
        "\n",
        "    # Make predictions\n",
        "    self_care_pred = model_self_care.predict(X_test)\n",
        "    therapy_pred = model_therapy.predict(X_test)\n",
        "\n",
        "    # Display classification reports\n",
        "    print(\"Self Care Advice Classification Report:\")\n",
        "    self_care_report = classification_report(y_self_care_test, self_care_pred, target_names=le_self_care.classes_)\n",
        "    print(self_care_report)\n",
        "    print(\"\\nSuggested Therapy Classification Report:\")\n",
        "    self_therapy_report = classification_report(y_therapy_test, therapy_pred, target_names=le_therapy.classes_)\n",
        "    print(self_therapy_report)\n",
        "    return model_self_care , model_therapy, le_diagnosis, le_symtoms, le_self_care, le_therapy, self_care_report, self_therapy_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initial Reports\n",
        "\n",
        "### Self Care Advice Classification Report\n",
        "\n",
        "| Self Care Advice       | Precision | Recall | F1-Score | Support |\n",
        "|------------------------|-----------|--------|----------|---------|\n",
        "| Breathing Exercises    | 0.20      | 0.17   | 0.18     | 166     |\n",
        "| Exercise               | 0.17      | 0.24   | 0.20     | 181     |\n",
        "| Journaling             | 0.32      | 0.14   | 0.20     | 191     |\n",
        "| Meditation             | 0.11      | 0.03   | 0.04     | 116     |\n",
        "| Take Breaks            | 0.17      | 0.33   | 0.23     | 164     |\n",
        "| Talk to a Friend       | 0.22      | 0.21   | 0.21     | 182     |\n",
        "| **Accuracy**           |           |        | 0.19     | 1000    |\n",
        "| **Macro Avg**          | 0.20      | 0.19   | 0.18     | 1000    |\n",
        "| **Weighted Avg**       | 0.21      | 0.19   | 0.18     | 1000    |\n",
        "\n",
        "### Suggested Therapy Classification Report\n",
        "\n",
        "| Suggested Therapy               | Precision | Recall | F1-Score | Support |\n",
        "|---------------------------------|-----------|--------|----------|---------|\n",
        "| Cognitive Behavioral Therapy    | 0.16      | 0.13   | 0.14     | 200     |\n",
        "| Mindfulness-Based Therapy       | 0.14      | 0.03   | 0.05     | 189     |\n",
        "| No Therapy Needed               | 0.17      | 0.09   | 0.11     | 187     |\n",
        "| Psychotherapy                   | 0.17      | 0.31   | 0.22     | 202     |\n",
        "| Support Groups                  | 0.24      | 0.34   | 0.28     | 222     |\n",
        "| **Accuracy**                    |           |        | 0.19     | 1000    |\n",
        "| **Macro Avg**                   | 0.17      | 0.18   | 0.16     | 1000    |\n",
        "| **Weighted Avg**                | 0.18      | 0.19   | 0.17     | 1000    |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model_self_care , model_therapy, le_diagnosis, le_symtoms, le_self_care, le_therapy, self_care_report, self_therapy_report = build_model()\n",
        "# Save model for purpose\n",
        "import joblib\n",
        "def save_model(model_self_care , model_therapy, le_diagnosis, le_symtoms, le_self_care, le_therapy ):\n",
        "    joblib.dump(model_self_care, \"model_self_care.pkl\")\n",
        "    joblib.dump(model_therapy, \"model_therapy.pkl\")\n",
        "    joblib.dump(le_diagnosis, \"le_diagnosis.pkl\")\n",
        "    joblib.dump(le_symtoms, \"le_symtoms.pkl\")\n",
        "    joblib.dump(le_self_care, \"le_self_care.pkl\")\n",
        "    joblib.dump(le_therapy, \"le_therapy.pkl\")\n",
        "\n",
        "# save_model(model_self_care , model_therapy, le_diagnosis, le_symtoms, le_self_care, le_therapy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Current accuracy 0.18 precison also,\n",
        "To imporve this models. <br>\n",
        "There are serveral ways to imporve ways.<br>\n",
        "Add more parameters (which contains demographic infomrationm which user can simply input them), also re mapping based Diagnosis.<br>\n",
        "I created three value includes 2 informaiton.<br>\n",
        "It wil help to organize better modeling. <br>\n",
        "Current features has <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Other' 'Female' 'Non-binary' 'Male']\n",
            "[29 37 47 35 22  8 31 20 21  9 38 43 30  2  3 34 36 19 33 23 46 49 48 41\n",
            " 11 16 10 45 13  4 39 12 51 24 17 32  1 14 44 15 26  5 40 27 42 18  6 50\n",
            " 28 25  7]\n",
            "['Moderate' 'High' 'Low' 'Critical']\n",
            "[ 1  4  5  2  6  9 10  8  7  3]\n",
            "['OCD' nan 'PTSD' 'Bipolar Disorder' 'Anxiety' 'Depression']\n"
          ]
        }
      ],
      "source": [
        "print(mental_df['Gender'].unique())\n",
        "print(mental_df['Duration'].unique())\n",
        "print(mental_df['Urgency_Level'].unique())\n",
        "print(mental_df['Stress_Level'].unique())\n",
        "print(mental_df['Prev_Diagnosis'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diagnosis Group Classification Report:\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "       Anxiety Disorders       0.46      0.63      0.53       422\n",
            "          Mood Disorders       0.47      0.40      0.44       440\n",
            "Stress-Related Disorders       0.16      0.06      0.09       138\n",
            "\n",
            "                accuracy                           0.45      1000\n",
            "               macro avg       0.37      0.36      0.35      1000\n",
            "            weighted avg       0.42      0.45      0.43      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Diagnosis Group\n",
        "\n",
        "def group_diagnosis(row):\n",
        "    if row['Diagnosis'] in ['Panic Disorder', 'Anxiety']:\n",
        "        return 'Anxiety Disorders'\n",
        "    elif row['Diagnosis'] in ['Depression', 'Burnout']:\n",
        "        return 'Mood Disorders'\n",
        "    elif row['Diagnosis'] == 'Stress':\n",
        "        return 'Stress-Related Disorders'\n",
        "def group_prev_diagnosis(row):\n",
        "    if row['Prev_Diagnosis'] in ['Panic Disorder', 'Anxiety', 'OCD']:\n",
        "        return 'Anxiety Disorders'\n",
        "    elif row['Prev_Diagnosis'] in ['Depression', 'Bipolar Disorder']:\n",
        "        return 'Mood Disorders'\n",
        "    elif row['Prev_Diagnosis'] in ['Stress', 'PTSD']:\n",
        "        return 'Stress-Related Disorders'\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "def re_map_gender(row):\n",
        "    if row['Gender'] == 'Male':\n",
        "        return 1\n",
        "    elif row['Gender'] == 'Female':\n",
        "        return 2\n",
        "    else:\n",
        "        return 3\n",
        "def re_map_urgency_level(row):\n",
        "    if row[\"Urgency_Level\"] == \"Low\":\n",
        "        return 1\n",
        "    elif row[\"Urgency_Level\"] == \"Moderate\":\n",
        "        return 2\n",
        "    elif row[\"Urgency_Level\"] == \"High\":\n",
        "        return 3\n",
        "    else:\n",
        "        return 4\n",
        "    \n",
        "# Inital diagnosis model\n",
        "def improved_test_reports_diagnosis(mental_df):\n",
        "    # Diagnosis Group\n",
        "    mental_df[\"Diagnosis_Group\"] = mental_df.apply(group_diagnosis, axis=1)\n",
        "    mental_df['Prev_Diagnosis_Group'] = mental_df.apply(group_prev_diagnosis, axis=1)\n",
        "    mental_df[\"Re_Gender\"] = mental_df.apply(re_map_gender, axis=1)\n",
        "    mental_df[\"Urgency_Level\"] = mental_df.apply(re_map_urgency_level, axis=1)\n",
        "    \n",
        "    le_diagnosis_group = LabelEncoder()\n",
        "    le_prev_Diagnosis_group = LabelEncoder()\n",
        "    le_symtoms = LabelEncoder()\n",
        "\n",
        "    mental_df['Diagnosis_Group_encoded'] = le_diagnosis_group.fit_transform(mental_df['Diagnosis_Group'])\n",
        "    mental_df['Prev_Diagnosis_Group_encoded'] =  le_prev_Diagnosis_group.fit_transform(mental_df['Prev_Diagnosis'])\n",
        "    mental_df['Symptoms_encoded'] = le_symtoms.fit_transform(mental_df['Symptoms'])\n",
        "\n",
        "    # Training Data with duration\n",
        "    X = mental_df[['Age', 'Symptoms_encoded', \"Re_Gender\", \"Prev_Diagnosis_Group_encoded\", \"Duration\", \"Stress_Level\", \"Urgency_Level\"]] \n",
        "    y_diagnosis = mental_df['Diagnosis_Group_encoded']\n",
        "\n",
        "    X_train, X_test, y_diagnosis_train, y_diagnosis_test = train_test_split(X, y_diagnosis, test_size=0.2, random_state=42)\n",
        "    \n",
        "    # Train model for Diagnosis\n",
        "    model_diagnosis = RandomForestClassifier()\n",
        "    model_diagnosis.fit(X_train, y_diagnosis_train)\n",
        "\n",
        "    # Make predictions\n",
        "    diagnos_pred = model_diagnosis.predict(X_test)\n",
        "\n",
        "    # Display classification reports\n",
        "    print(\"Diagnosis Group Classification Report:\")\n",
        "    diagnosis_report = classification_report(y_diagnosis_test, diagnos_pred, target_names=le_diagnosis_group.classes_)\n",
        "    print(diagnosis_report)\n",
        "    \n",
        "    return model_diagnosis, le_diagnosis_group, le_prev_Diagnosis_group, le_symtoms, diagnosis_report\n",
        "\n",
        "model_diagnosis, le_diagnosis_group, le_prev_Diagnosis_group, le_symtoms, diagnosis_report = improved_test_reports_diagnosis(mental_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to diagnosisModel\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Download Model\n",
        "def createModel(model_diagnosis, le_diagnosis_group, le_prev_Diagnosis_group, le_symtoms):\n",
        "      # Save the model to the diagnosisModel folder\n",
        "    os.makedirs('diagnosisModel', exist_ok=True)\n",
        "    model_path = 'diagnosisModel'\n",
        "    joblib.dump(model_diagnosis, 'diagnosisModel/diagnosis_model.pkl')\n",
        "    joblib.dump(le_diagnosis_group, 'diagnosisModel/le_diagnosis_group.pkl')\n",
        "    joblib.dump(le_prev_Diagnosis_group, 'diagnosisModel/le_prev_Diagnosis_group.pkl')\n",
        "    joblib.dump(le_symtoms, 'diagnosisModel/le_symptoms.pkl')\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "    \n",
        "createModel(model_diagnosis, le_diagnosis_group, le_prev_Diagnosis_group, le_symtoms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diagnosis Group Classification Report (SVM):\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "       Anxiety Disorders       0.42      1.00      0.59       422\n",
            "          Mood Disorders       0.00      0.00      0.00       440\n",
            "Stress-Related Disorders       0.00      0.00      0.00       138\n",
            "\n",
            "                accuracy                           0.42      1000\n",
            "               macro avg       0.14      0.33      0.20      1000\n",
            "            weighted avg       0.18      0.42      0.25      1000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(SVC(kernel='poly'),\n",
              " LabelEncoder(),\n",
              " '                          precision    recall  f1-score   support\\n\\n       Anxiety Disorders       0.42      1.00      0.59       422\\n          Mood Disorders       0.00      0.00      0.00       440\\nStress-Related Disorders       0.00      0.00      0.00       138\\n\\n                accuracy                           0.42      1000\\n               macro avg       0.14      0.33      0.20      1000\\n            weighted avg       0.18      0.42      0.25      1000\\n')"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "def improved_test_reports_diagnosis_svm(mental_df):\n",
        "    # Same preprocessing steps\n",
        "    mental_df[\"Diagnosis_Group\"] = mental_df.apply(group_diagnosis, axis=1)\n",
        "    mental_df['Prev_Diagnosis_Group'] = mental_df.apply(group_prev_diagnosis, axis=1)\n",
        "    mental_df[\"Re_Gender\"] = mental_df.apply(re_map_gender, axis=1)\n",
        "    mental_df[\"Urgency_Level\"] = mental_df.apply(re_map_urgency_level, axis=1)\n",
        "\n",
        "    le_diagnosis_group = LabelEncoder()\n",
        "    le_prev_Diagnosis_group = LabelEncoder()\n",
        "    le_symtoms = LabelEncoder()\n",
        "\n",
        "    mental_df['Diagnosis_Group_encoded'] = le_diagnosis_group.fit_transform(mental_df['Diagnosis_Group'])\n",
        "    mental_df['Prev_Diagnosis_Group_encoded'] = le_prev_Diagnosis_group.fit_transform(mental_df['Prev_Diagnosis'])\n",
        "    mental_df['Symptoms_encoded'] = le_symtoms.fit_transform(mental_df['Symptoms'])\n",
        "\n",
        "    X = mental_df[['Age', 'Symptoms_encoded', \"Re_Gender\", \"Prev_Diagnosis_Group_encoded\", \"Duration\", \"Stress_Level\", \"Urgency_Level\"]] \n",
        "    y_diagnosis = mental_df['Diagnosis_Group_encoded']\n",
        "\n",
        "    X_train, X_test, y_diagnosis_train, y_diagnosis_test = train_test_split(X, y_diagnosis, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train model using Support Vector Machine\n",
        "    model_diagnosis = SVC(kernel='poly')  # You can also try 'rbf' or 'poly'\n",
        "    model_diagnosis.fit(X_train, y_diagnosis_train)\n",
        "\n",
        "    # Make predictions\n",
        "    diagnos_pred = model_diagnosis.predict(X_test)\n",
        "\n",
        "    # Display classification report\n",
        "    print(\"Diagnosis Group Classification Report (SVM):\")\n",
        "    diagnosis_report = classification_report(y_diagnosis_test, diagnos_pred, target_names=le_diagnosis_group.classes_)\n",
        "    print(diagnosis_report)\n",
        "\n",
        "    return model_diagnosis, le_diagnosis_group, diagnosis_report\n",
        "\n",
        "improved_test_reports_diagnosis_svm(mental_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
            "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
            "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
          ]
        },
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/yoon/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/Users/yoon/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[34], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m related_words\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Get the related words and print the dictionary\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m similar_words_dict \u001b[38;5;241m=\u001b[39m \u001b[43mfind_related_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(similar_words_dict)\n",
            "Cell \u001b[0;32mIn[34], line 21\u001b[0m, in \u001b[0;36mfind_related_words\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m related_words \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, word \u001b[38;5;129;01min\u001b[39;00m words\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 21\u001b[0m     word_synsets \u001b[38;5;241m=\u001b[39m \u001b[43mwordnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynsets\u001b[49m(word)\n\u001b[1;32m     22\u001b[0m     similar_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(chain(\u001b[38;5;241m*\u001b[39m[synset\u001b[38;5;241m.\u001b[39mlemma_names() \u001b[38;5;28;01mfor\u001b[39;00m synset \u001b[38;5;129;01min\u001b[39;00m word_synsets]))\n\u001b[1;32m     23\u001b[0m     related_words[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(similar_words)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/corpus/util.py:120\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/yoon/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.12/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from itertools import chain\n",
        "\n",
        "# Based on questions and topics, Create chains then finds vlaues.\n",
        "def find_related_words():\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "    words = {\n",
        "        'Panic': 'panic',\n",
        "        'Disorder': 'disorder',\n",
        "        'Depression': 'depression',\n",
        "        'Burnout': 'burnout',\n",
        "        'Stress': 'stress'\n",
        "    }\n",
        "\n",
        "    # Initialize a dictionary to store the results\n",
        "    related_words = {}\n",
        "\n",
        "    for key, word in words.items():\n",
        "        word_synsets = wordnet.synsets(word)\n",
        "        similar_words = set(chain(*[synset.lemma_names() for synset in word_synsets]))\n",
        "        related_words[key] = list(similar_words)\n",
        "\n",
        "    return related_words\n",
        "\n",
        "# Get the related words and print the dictionary\n",
        "similar_words_dict = find_related_words()\n",
        "print(similar_words_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Result for related words\n",
        "# {'Panic': ['panic', 'scare', 'terror', 'affright'], \n",
        "# 'Disorder': ['distract', 'upset', 'trouble', 'disorderliness', 'cark', 'disarray', 'disorder', 'perturb', 'disquiet', 'unhinge'], \n",
        "# 'Depression': ['impression', 'clinical_depression', 'slump', 'Great_Depression', 'Depression', 'low', 'depressive_disorder', 'natural_depression', 'imprint', 'economic_crisis', 'depression'], \n",
        "# 'Burnout': [], \n",
        "# 'Stress': ['strain', 'emphasis', 'emphasize', 'tension', 'punctuate', 'focus', 'accentuate', 'try', 'tenseness', 'emphasise', 'stress', 'accent']}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Counsel data training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Orignal length1482\n",
            "\n",
            "Word frequencies in the 'topics' column:\n",
            "Family :  119\n",
            "Conflict :  91\n",
            "Substance :  14\n",
            "AbuseAddiction :  9\n",
            "Behavioral :  49\n",
            "ChangeSocial :  3\n",
            "Relationships :  214\n",
            "Relationship :  45\n",
            "Dissolution :  68\n",
            "Anger :  32\n",
            "Management :  25\n",
            "Sleep :  10\n",
            "Improvement :  17\n",
            "Professional :  34\n",
            "EthicsLegal :  6\n",
            "& :  45\n",
            "Regulatory :  20\n",
            "Social :  41\n",
            "RelationshipsMarriage :  11\n",
            "MarriageIntimacy :  26\n",
            "Domestic :  9\n",
            "ViolenceAnger :  2\n",
            "ManagementFamily :  3\n",
            "Human :  50\n",
            "Sexuality :  33\n",
            "ManagementSleep :  2\n",
            "Military :  3\n",
            "Issues :  10\n",
            "RelationshipsDomestic :  3\n",
            "Violence :  10\n",
            "ViolenceRelationship :  1\n",
            "Marriage :  25\n",
            "Grief :  20\n",
            "and :  23\n",
            "Loss :  9\n",
            "ConflictChildren :  1\n",
            "Adolescents :  9\n",
            "MarriageRelationship :  4\n",
            "TraumaHuman :  1\n",
            "RelationshipsIntimacy :  42\n",
            "ManagementParenting :  1\n",
            "Intimacy :  17\n",
            "Workplace :  9\n",
            "SexualityMarriage :  3\n",
            "LGBTQ :  29\n",
            "SpiritualityFamily :  2\n",
            "Ethics :  23\n",
            "ViolenceRelationships :  1\n",
            "ConflictRelationships :  5\n",
            "Self-esteem :  29\n",
            "Self-esteemRelationships :  12\n",
            "Parenting :  35\n",
            "ConflictMarriage :  6\n",
            "ConflictSelf-esteem :  3\n",
            "ParentingRelationships :  1\n",
            "nan :  10\n",
            "Counseling :  53\n",
            "RelationshipsSelf-esteem :  9\n",
            "Eating :  7\n",
            "RelationshipsProfessional :  2\n",
            "ParentingSubstance :  1\n",
            "AbuseSpirituality :  1\n",
            "Self-esteemRelationship :  1\n",
            "ConflictAnger :  2\n",
            "ParentingAnger :  1\n",
            "MarriageFamily :  3\n",
            "ConflictProfessional :  2\n",
            "RelationshipsHuman :  4\n",
            "SexualityLGBTQ :  10\n",
            "RelationshipsParentingFamily :  1\n",
            "Legal :  16\n",
            "LGBTQIntimacy :  1\n",
            "ManagementRelationships :  3\n",
            "AbuseFamily :  1\n",
            "Self-esteemMarriageTraumaIntimacy :  3\n",
            "MarriageAddiction :  1\n",
            "RelationshipsLegal :  1\n",
            "SexualityRelationships :  4\n",
            "ConflictRelationshipsMarriage :  1\n",
            "MarriageAnger :  1\n",
            "RelationshipsFamily :  8\n",
            "Change :  56\n",
            "SexualitySocial :  1\n",
            "Self-esteemEating :  1\n",
            "Career :  2\n",
            "CounselingProfessional :  2\n",
            "MarriageGrief :  1\n",
            "Self-esteemSocial :  5\n",
            "AddictionSubstance :  11\n",
            "Abuse :  23\n",
            "Spirituality :  8\n",
            "RelationshipsSocial :  22\n",
            "SexualityAddiction :  2\n",
            "IntimacyRelationships :  11\n",
            "RelationshipsSelf-esteemHuman :  1\n",
            "Trauma :  28\n",
            "SexualityIntimacyMarriage :  9\n",
            "RelationshipsParenting :  1\n",
            "ConflictParenting :  16\n",
            "RegulatoryProfessional :  3\n",
            "ManagementDomestic :  2\n",
            "ParentingFamily :  6\n",
            "ConflictLegal :  6\n",
            "ViolenceLegal :  1\n",
            "IntimacyHuman :  7\n",
            "IntimacySocial :  1\n",
            "TraumaMilitary :  3\n",
            "ManagementRelationshipsSocial :  3\n",
            "TraumaFamily :  4\n",
            "TraumaSelf-esteemRelationship :  1\n",
            "RelationshipsRelationship :  2\n",
            "RelationshipsBehavioral :  6\n",
            "MarriageDomestic :  1\n",
            "ConflictParentingMarriage :  1\n",
            "MarriageIntimacyHuman :  9\n",
            "MarriageRelationshipsIntimacy :  9\n",
            "SexualityFamily :  1\n",
            "ConflictSpirituality :  1\n",
            "RelationshipsIntimacyHuman :  1\n",
            "ParentingRelationship :  9\n",
            "RelationshipsTrauma :  1\n",
            "AddictionMarriageIntimacy :  3\n",
            "ConflictTrauma :  1\n",
            "ConflictSocial :  1\n",
            "RelationshipsRelationshipsIntimacy :  1\n",
            "LGBTQRelationshipsIntimacy :  1\n",
            "MarriageSocial :  1\n",
            "ManagementSocial :  1\n",
            "RelationshipsRelationships :  1\n",
            "ChangeMarriage :  1\n",
            "ManagementBehavioral :  1\n",
            "ViolenceMarriage :  1\n",
            "LGBTQFamily :  4\n",
            "IntimacyMarriage :  1\n",
            "ConflictRelationshipsIntimacy :  1\n",
            "ConflictLGBTQ :  6\n",
            "SpiritualityRelationships :  2\n",
            "RelationshipsWorkplace :  5\n",
            "SexualityIntimacyRelationships :  14\n",
            "SexualityIntimacy :  8\n",
            "RegulatoryAddiction :  2\n",
            "RelationshipsSubstance :  8\n",
            "Self-esteemSleep :  1\n",
            "RelationshipsChildren :  1\n",
            "ChangeLGBTQ :  1\n",
            "LossFamily :  2\n",
            "Self-esteemBehavioral :  2\n",
            "RelationshipsRelationshipsAddiction :  1\n",
            "DiagnosisCounseling :  4\n",
            "Fundamentals :  87\n",
            "RelationshipsLGBTQ :  2\n",
            "Self-esteemLGBTQ :  1\n",
            "EthicsParentingLegal :  1\n",
            "TraumaRelationships :  8\n",
            "LGBTQHuman :  4\n",
            "IntimacyTrauma :  2\n",
            "ViolenceSleep :  2\n",
            "LossSubstance :  7\n",
            "AbuseTrauma :  7\n",
            "IntimacyRelationshipsHuman :  11\n",
            "RelationshipsCareer :  2\n",
            "ChangeRelationships :  1\n",
            "Addiction :  3\n",
            "ConflictDomestic :  1\n",
            "Alzheimer'sFamily :  2\n",
            "SexualityRelationshipsIntimacy :  2\n",
            "ChangeSleep :  1\n",
            "SpiritualitySocial :  6\n",
            "RelationshipsIntimacyLGBTQ :  5\n",
            "MarriageHuman :  2\n",
            "MarriageIntimacyAddictionBehavioral :  1\n",
            "LossRelationships :  2\n",
            "ConflictRelationship :  1\n",
            "IntimacyRelationshipsDomestic :  1\n",
            "AddictionProfessional :  2\n",
            "ConflictParentingRelationship :  1\n",
            "Diagnosis :  8\n",
            "RegulatoryParentingFamily :  5\n",
            "AbuseSocial :  4\n",
            "RelationshipsAddictionSubstance :  2\n",
            "AbuseSelf-esteem :  1\n",
            "Self-esteemSpirituality :  1\n",
            "SexualityTrauma :  1\n",
            "SexualityTraumaIntimacyRelationships :  1\n",
            "LossTrauma :  1\n",
            "Self-esteemSubstance :  1\n",
            "RelationshipsMarriageWorkplace :  3\n",
            "RelationshipsMilitary :  4\n",
            "RegulatorySubstance :  5\n",
            "ConflictRelationshipsParenting :  2\n",
            "ManagementRelationshipsSubstance :  1\n",
            "RelationshipsIntimacySpirituality :  5\n",
            "SexualityLGBTQIntimacy :  3\n",
            "ParentingChildren :  4\n",
            "Children :  3\n",
            "EthicsCounseling :  15\n",
            "ConflictRelationshipsRelationship :  1\n",
            "AdolescentsBehavioral :  1\n",
            "RelationshipsAddiction :  2\n",
            "ConflictAddictionSubstance :  1\n",
            "ParentingAddiction :  1\n",
            "ManagementSelf-esteemMarriageFamily :  1\n",
            "ConflictParentingChildren :  1\n",
            "total counts1482\n",
            "stress_count 54\n",
            "depression_count 196\n",
            "disorder_count 12\n",
            "anxiety_count 180\n",
            "burn_out_count 0\n",
            "Found selcted count 442\n"
          ]
        }
      ],
      "source": [
        "counsel_df = pd.read_csv(\"counselchat-data.csv\")\n",
        "\n",
        "from collections import Counter\n",
        "# Ensure you've downloaded the WordNet corpus\n",
        "# Display the word frequencies\n",
        "def get_word_frequencies(counsel_df):\n",
        "    print(\"Orignal length\" + str(len(counsel_df)))\n",
        "    counsel_df = counsel_df[['questionText', 'topics','answerText']]\n",
        "    all_words = ' '.join(counsel_df['topics'].astype(str)).replace(',', '').split()\n",
        "    # Count the frequency of each word\n",
        "    word_count = Counter(all_words)\n",
        "    print(\"\\nWord frequencies in the 'topics' column:\")\n",
        "    found_selected_count = 0\n",
        "    stress_count = 0\n",
        "    depression_count = 0\n",
        "    disorder_count = 0\n",
        "    anxiety_count = 0\n",
        "    burn_out_count = 0\n",
        "    for word, count in word_count.items():\n",
        "        if word.__contains__(\"Stress\"):\n",
        "            stress_count +=count\n",
        "        elif word.__contains__(\"Depression\"):\n",
        "            depression_count += count\n",
        "        elif word.__contains__(\"Disorder\"):\n",
        "            disorder_count += count\n",
        "        elif word.__contains__(\"Anxiety\"):\n",
        "            anxiety_count += count\n",
        "        elif word.__contains__(\"Burnout\"):\n",
        "            burn_out_count += count\n",
        "        else:\n",
        "            print(f\"{word} :  {count}\")\n",
        "    found_selected_count = stress_count + depression_count + disorder_count + anxiety_count + burn_out_count\n",
        "    print(\"total counts\" + str(len(counsel_df)))\n",
        "    print(\"stress_count\", stress_count)\n",
        "    print(\"depression_count\", depression_count)\n",
        "    print(\"disorder_count\", disorder_count)\n",
        "    print(\"anxiety_count\", anxiety_count)\n",
        "    print(\"burn_out_count\", burn_out_count)\n",
        "    print(\"Found selcted count\", found_selected_count)\n",
        "\n",
        "get_word_frequencies(counsel_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.9976133651551312\n",
            "Number of rows in test set: 1063\n"
          ]
        }
      ],
      "source": [
        "# https://my.clevelandclinic.org/health/diseases/22295-mental-health-disorders\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "counsel_df = pd.read_csv(\"counselchat-data.csv\")\n",
        "target_Keywords = {\n",
        "    'Anxiety Disorders': ['panic disorder', 'anxiety'],\n",
        "    'Mood Disorders': ['depression', 'burnout'],\n",
        "    'Stress-Related Disorders': ['stress', 'PTSD']\n",
        "}\n",
        "\n",
        "similar_words_dict = {\n",
        "    'panic': ['panic', 'scare', 'terror', 'affright'],\n",
        "    'disorder': ['distract', 'upset', 'trouble', 'disorderliness', 'cark', 'disarray', 'disorder', 'perturb', 'disquiet', 'unhinge'],\n",
        "    'depression': ['impression', 'clinical_depression', 'slump', 'Great_Depression', 'depression', 'low', 'depressive_disorder', 'natural_depression', 'imprint', 'economic_crisis', 'depression'],\n",
        "    'burnout': [],\n",
        "    'stress': ['strain', 'emphasis', 'emphasize', 'tension', 'punctuate', 'focus', 'accentuate', 'try', 'tenseness', 'emphasise', 'stress', 'accent']\n",
        "}\n",
        "\n",
        "def group_diagnosis(row):\n",
        "    all_words = str(row['topics']).lower().split()\n",
        "\n",
        "    for disorder, keywords in target_Keywords.items():\n",
        "        for keyword in keywords:\n",
        "            if any(keyword in word for word in all_words):\n",
        "                return disorder\n",
        "            if keyword in similar_words_dict:\n",
        "                # Check if any of the similar words match\n",
        "                if any(sim_word in all_words for sim_word in similar_words_dict[keyword]):\n",
        "                    return disorder\n",
        "    return None\n",
        "# Original 120.\n",
        "# print(similar_words_dict)\n",
        "# Adding new counsel_df based on items\n",
        "counsel_df[\"re_diagnosis\"] = counsel_df.apply(group_diagnosis, axis=1)\n",
        "# print(counsel_df['re_diagnosis'])\n",
        "# print(len(counsel_df['re_diagnosis']))\n",
        "\n",
        "# cd remapping diagnosis.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Function to create and train the model\n",
        "# TfidfVectorizer\n",
        "# LogicRegression\n",
        "def create_diagnosis_model(train_data, target_column='re_diagnosis'):\n",
        "    # Extract text and target columns\n",
        "    X_train = train_data['topics']\n",
        "    y_train = train_data[target_column]\n",
        "\n",
        "    # TF-IDF vectorization\n",
        "    tfidf = TfidfVectorizer(max_features=5000)\n",
        "    X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "\n",
        "    # Train a logistic regression model\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    # Evaluate model accuracy on the training set\n",
        "    X_train_pred = model.predict(X_train_tfidf)\n",
        "    accuracy = accuracy_score(y_train, X_train_pred)\n",
        "    print(f\"Training Accuracy: {accuracy}\")\n",
        "\n",
        "    return model, tfidf\n",
        "\n",
        "# Function to make predictions using the trained model\n",
        "def predict_missing_diagnoses(df, model, tfidf, target_column='re_diagnosis'):\n",
        "    # Extarct out df re_diagnosis is None\n",
        "    df_test = df[df[target_column].isna()]\n",
        "\n",
        "    X_test = df_test['topics']\n",
        "    print(f\"Number of rows in test set: {len(X_test)}\")\n",
        "    X_test = df_test['topics'].fillna('')  # Replace NaN values with an empty string\n",
        "    non_empty_mask = X_test.str.strip() != ''\n",
        "\n",
        "    #  \n",
        "    df_test = df_test[non_empty_mask]\n",
        "    X_test =  X_test[non_empty_mask]\n",
        "\n",
        "    # \n",
        "    X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "    # Predict missing diagnoses\n",
        "    predictions = model.predict(X_test_tfidf)\n",
        "\n",
        "    # Assign predictions back to the DataFrame\n",
        "    df.loc[df[target_column].isna() & non_empty_mask, target_column] = predictions\n",
        "\n",
        "    return df\n",
        "\n",
        "# Example usage\n",
        "# Step 1: Create model\n",
        "# Training and prediction process\n",
        "df_train = counsel_df[counsel_df['re_diagnosis'].notna()]\n",
        "model, tfidf = create_diagnosis_model(df_train)\n",
        "remapped_consel_df = predict_missing_diagnoses(counsel_df, model, tfidf)\n",
        "\n",
        "\n",
        "\n",
        "# remapped_consel_df.to_csv(\"remapped_consel_df\", index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment path: /usr/local/bin/python3\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(\"Environment path:\", sys.executable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_sentence(row):\n",
        "    soup = BeautifulSoup(row['answerText'], 'html.parser')\n",
        "    clean_text = soup.get_text() \n",
        "    return clean_text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1482\n",
            "Index(['questionText', 'topics', 're_diagnosis', 'cleaned_test'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(len(remapped_consel_df))\n",
        "print(remapped_consel_df.columns)\n",
        "consel_df = pd.read_csv(\"remapped_consel_df.csv\")\n",
        "consel_df[\"clean_answer_text\"] = consel_df.apply(extract_sentence, axis=1)\n",
        "consel_df = consel_df.dropna(subset=['questionText', 'topics', 're_diagnosis', 'clean_answer_text'])\n",
        "consel_df = consel_df[consel_df['questionText'].str.strip() != '']\n",
        "consel_df = consel_df[consel_df['topics'].str.strip() != '']\n",
        "consel_df = consel_df[consel_df['re_diagnosis'].str.strip() != '']\n",
        "consel_df = consel_df[consel_df['clean_answer_text'].str.strip() != '']\n",
        "\n",
        "consel_df = consel_df[['questionText', 'topics', 're_diagnosis', 'clean_answer_text']]\n",
        "consel_df.to_csv(\"counsel_cleaned.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inputs: \n",
            "\n",
            "Maximum token length: 659\n",
            "Average token length: 77.47050254916242\n",
            "95th percentile token length: 190\n",
            "Total numbers of target_df 1373\n",
            "After drop target_df 1299\n",
            "Maximum token length after drop: 185\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1138 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Outputs line as well \n",
            "\n",
            "Maximum token length: 1138\n",
            "Average token length: 229.5604311008468\n",
            "95th percentile token length: 559\n",
            "Total numbers of target_df 1299\n",
            "After drop target_df 1234\n",
            "Maximum token length after drop: 558\n",
            "Final numbers of target df 1234\n"
          ]
        }
      ],
      "source": [
        "# We only use this varible only.\n",
        "import numpy as np\n",
        "target_df = pd.read_csv(\"counsel_cleaned.csv\")\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Tokenize the inputs and get their lengths\n",
        "def clean_inputs_as_well(target_df):\n",
        "    target_df[\"inputs_text\"] = \"Question: \" + target_df[\"questionText\"] + \"Diagnosis: \" + target_df[\"re_diagnosis\"]\n",
        "    target_df[\"token_length\"] = target_df[\"inputs_text\"].apply(lambda text: len(tokenizer.encode(text, truncation=False)))\n",
        "\n",
        "    input_texts = (\"Question: \" + target_df[\"questionText\"] + \"Diagnosis: \" + target_df[\"re_diagnosis\"]).tolist()\n",
        "    token_lengths = [len(tokenizer.encode(text, truncation=False)) for text in input_texts]\n",
        "\n",
        "    # Calculate statistics\n",
        "    max_length = max(token_lengths)\n",
        "    avg_length = sum(token_lengths) / len(token_lengths)\n",
        "    percentile_95 = int(np.percentile(token_lengths, 95))  # 95th percentile length\n",
        "    print(\"Inputs: \\n\")\n",
        "    print(f\"Maximum token length: {max_length}\")\n",
        "    print(f\"Average token length: {avg_length}\")\n",
        "    print(f\"95th percentile token length: {percentile_95}\")\n",
        "    print(f\"Total numbers of target_df {len(target_df)}\")\n",
        "    target_df = target_df[target_df[\"token_length\"] < 190]\n",
        "    print(f\"After drop target_df {len(target_df)}\")\n",
        "    max_length_after_drop = target_df[\"token_length\"].max()\n",
        "    print(f\"Maximum token length after drop: {max_length_after_drop}\")\n",
        "    return target_df\n",
        "\n",
        "def clean_outputs_as_well(target_df):\n",
        "    target_df[\"outputs_text\"] = target_df['clean_answer_text']\n",
        "    target_df[\"token_length\"] = target_df[\"outputs_text\"].apply(lambda text: len(tokenizer.encode(text, truncation=False)))\n",
        "\n",
        "    outputs_text = (target_df['outputs_text']).tolist()\n",
        "    token_lengths = [len(tokenizer.encode(text, truncation=False)) for text in outputs_text]\n",
        "\n",
        "    # Calculate statistics\n",
        "    max_length = max(token_lengths)\n",
        "    avg_length = sum(token_lengths) / len(token_lengths)\n",
        "    percentile_95 = int(np.percentile(token_lengths, 95))  # 95th percentile length\n",
        "    print(\"Outputs line as well \\n\")\n",
        "    print(f\"Maximum token length: {max_length}\")\n",
        "    print(f\"Average token length: {avg_length}\")\n",
        "    print(f\"95th percentile token length: {percentile_95}\")\n",
        "    print(f\"Total numbers of target_df {len(target_df)}\")\n",
        "    target_df = target_df[target_df[\"token_length\"] < percentile_95]\n",
        "    print(f\"After drop target_df {len(target_df)}\")\n",
        "    max_length_after_drop = target_df[\"token_length\"].max()\n",
        "    print(f\"Maximum token length after drop: {max_length_after_drop}\")\n",
        "    return target_df\n",
        "\n",
        "target_df = clean_inputs_as_well(target_df)\n",
        "target_df = clean_outputs_as_well(target_df)\n",
        "\n",
        "print(f\"Final numbers of target df {len(target_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['questionText', 'topics', 're_diagnosis', 'clean_answer_text',\n",
              "       'inputs_text', 'token_length', 'outputs_text'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and tokenizer saved in 'max_token_updated'\n"
          ]
        }
      ],
      "source": [
        "# Finally, train NLP gpt3.\n",
        "# Starting from \n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Load and preprocess data\n",
        "# train_df = pd.read_csv('remapped_consel_df.csv')\n",
        "train_df = target_df\n",
        "train_df[\"input_text\"] = \"Question: \" + train_df[\"questionText\"] + \"Diagnosis: \" + train_df[\"re_diagnosis\"]\n",
        "train_df[\"target_text\"] = train_df[\"clean_answer_text\"]\n",
        "\n",
        "# Initialize the model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Set padding token for GPT2, which does not have one by default\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Token Based counts\n",
        "\n",
        "class ChatBotDataSet(Dataset):\n",
        "    def __init__(self, tokenizer, input_texts, target_texts, max_length_inputs=185, max_length_outputs = 558):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_texts = input_texts\n",
        "        self.target_texts = target_texts\n",
        "        self.max_length_inputs = max_length_inputs\n",
        "        self.max_length_outputs = max_length_outputs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Encode input and target texts with truncation and padding\n",
        "        input_encodings = self.tokenizer(\n",
        "            self.input_texts[index],\n",
        "            max_length=self.max_length_inputs,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        target_encodings = self.tokenizer(\n",
        "            self.target_texts[index],\n",
        "            max_length=self.max_length_outputs,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_encodings[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": input_encodings[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": target_encodings[\"input_ids\"].squeeze(),\n",
        "        }\n",
        "\n",
        "# Prepare dataset and dataloader\n",
        "input_texts = train_df[\"input_text\"].tolist()\n",
        "target_texts = train_df[\"target_text\"].tolist()\n",
        "dataset = ChatBotDataSet(tokenizer, input_texts, target_texts)\n",
        "train_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Model training setup\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "model.train()\n",
        "\n",
        "save_directory = \"max_token_updated\"\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "\n",
        "print(f\"Model and tokenizer saved in '{save_directory}'\")\n",
        "\n",
        "# for epoch in range(3):  # Adjust epochs as needed\n",
        "#     for batch in train_loader:\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(\n",
        "#             input_ids=batch[\"input_ids\"],\n",
        "#             attention_mask=batch[\"attention_mask\"],\n",
        "#             labels=batch[\"labels\"]\n",
        "#         )\n",
        "#         loss = outputs.loss\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Response: \n",
            " Question:I failed my exam. I think I am going to always fails. Diagnosis: Mood Disorders _______________\n",
            "\n",
            "I failed my exam. I think I am going to always fails. Diagnosis: Mood Disorders _______________\n",
            "\n",
            "I failed my exam. I think I am going to always fails. Diagnosis: Mood Disorders _______________\n",
            "\n",
            "I failed my exam. I think I am going to always fails. Diagnosis: Mood Disorders _______________\n",
            "\n",
            "I failed\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(filename='/Users/yoon/Desktop/MySchool/GeorgiaTech-Assingment/Untitled/model_response_log.txt', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
        "\n",
        "\n",
        "# Load the tokenizer and the model\n",
        "model_name = \"max_token_updated\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Example input\n",
        "input_text = \"\"\"Question:I failed my exam. I think I am going to always fails. Diagnosis: Mood Disorders \"\"\"\n",
        "inputs = tokenizer.encode(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "attention_mask = torch.ones_like(inputs)\n",
        "\n",
        "# Generate the response\n",
        "outputs = model.generate(\n",
        "    inputs,\n",
        "    max_length=100,\n",
        "    num_return_sequences=1,\n",
        "    attention_mask=attention_mask,\n",
        "    temperature=0.7,      \n",
        "    top_k=50,            \n",
        "    top_p=0.9,             \n",
        "    do_sample=True         \n",
        ")\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Run a forward pass to ensure the model is working\n",
        "# with torch.no_grad():\n",
        "#     outputs = model(inputs)\n",
        "#     print(\"Forward pass successful. Model output shape:\", outputs.logits.shape)\n",
        "\n",
        "# Display the response\n",
        "print(\"Generated Response: \\n\", response)\n",
        "# Log the input question, model used, and response\n",
        "logging.info(f\"Model used: {model_name}\")\n",
        "logging.info(f\"Input Question: {input_text}\")\n",
        "logging.info(f\"Generated Response: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Working Directory: /Users/yoon/Desktop/MySchool/GeorgiaTech-Assingment/Untitled\n",
            "Logging setup complete.\n"
          ]
        }
      ],
      "source": [
        "# Configure logging\n",
        "import logging\n",
        "import os\n",
        "\n",
        "print(\"Current Working Directory:\", os.getcwd())\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename='test_log_file.txt', \n",
        "    level=logging.INFO, \n",
        "    format='%(asctime)s - %(message)s'\n",
        ")\n",
        "\n",
        "logging.info(\"This is a test log message.\")\n",
        "print(\"Logging setup complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
